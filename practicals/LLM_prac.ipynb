{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# LLMs for everyone\n",
        "\n",
        "<img src=\"https://www.marktechpost.com/wp-content/uploads/2023/05/Blog-Banner-3.jpg\" width=\"60%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/Indaba_2023_Prac_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "Â© Deep Learning Indaba 2023. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "\n",
        "**Reviewers:**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "[Intro to the topic. Probably between 3-5 lines.]\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: [Attention Mechanism, Transformer Architecture, LoRA & QLoRA, RLHF; Multimodal Transformers; and other relevant developments related to LLMs]\n",
        "Level: [<font color='orange'>`Beginner`</font>, <font color='green'>`Intermediate`</font>, <font color='blue'>`Advanced`</font>, <font color='blue'>`Advanced`</font>]\n",
        "\n",
        "[Let's use the colours from notion here. E.g. <font color='grey'>`Beginner`</font> and <font color='blue'>`Generative Models`</font>.]\n",
        "\n",
        "\n",
        "*TODO*: Change to correct colour and format.\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "* Understanding attention mechanishms and why they are used\n",
        "* Understand and implement the fundamental building blocks of the Transformer Architecture\n",
        "* Finetuning a LLM (7 billion paramters) on a single GPU using PEFT techniques, speficially LoRA\n",
        "* Introduce references to trending research and applications of LLMs\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "* To populate still\n",
        "\n",
        "**Outline:**\n",
        "\n",
        "[Points that link to each section. Auto-generate following the instructions [here](https://stackoverflow.com/questions/67458990/how-to-automatically-generate-a-table-of-contents-in-colab-notebook).]\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n",
        "[Any other tasks just before starting.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952qogb79nnY"
      },
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |\n",
        "`Advanced`      | I work in this area/topic daily. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBdDHcI_ArCR",
        "outputId": "05b27949-c4c8-407c-dba8-84df7bb0fe8e"
      },
      "outputs": [],
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"advanced\" #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "if experience == \"beginner\":\n",
        "  sections_to_follow=\"Introduction -> 1.1 Subsection -> 2.1 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"intermediate\":\n",
        "  sections_to_follow=\"Introduction -> 1.2 Subsection -> 2.2 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"advanced\":\n",
        "  sections_to_follow=\"Introduction -> 1.3 Subsection -> 2.3 Subsection -> Conclusion -> Feedback\"\n",
        "\n",
        "print(f\"Based on your experience, it is advised you follow these -- {sections_to_follow} sections. Note this is just a guideline.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions. (Run Cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **1. Attention**\n",
        "\n",
        "[BASICALLY COPY PASTE FROM OLD PRAC]. This should be covered in full during the practical and must follow a structure that allows that. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **2. Deep dive into transformer**\n",
        "\n",
        "[Basically copy paste the old prac, but change the focus to focus on DECODER only, and still include the image processing step].\n",
        "\n",
        "The idea here is to build from scratch again. Theory and text can stay the same, but bit less code written perhaps, and focus on building only the **decoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA_2coZvhAg3"
      },
      "source": [
        "### 2.1 Highlevel overvierw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbTsk0MdhAhC"
      },
      "source": [
        "### 2.2 Tokenization + Positional encoding\n",
        "\n",
        "#### 2.2.1 Processing image (advanced section)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdNPg0pnhAhG"
      },
      "source": [
        "### 2.3 Transformer block\n",
        "\n",
        "#### 2.3.1 FFN\n",
        "\n",
        "#### 2.3.2 Norm block\n",
        "\n",
        "#### 2.3.3 Putting it together (FFN(norm(MHA(norm(X)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt3tp38G90A"
      },
      "source": [
        "### 2.4 Building LLM (decoder only transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXmjUYdDHseM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4hKnTFbHtdM"
      },
      "source": [
        "## 3. **Customising LLMs**\n",
        "\n",
        "* Cover openssource models and what hugginface is\n",
        "* Introduce architectures like LLAMA and FALCON\n",
        "* Mention that these are foundation models, and for many use cases you will have to fine tune the model\n",
        "* Bring in these model sizes are not easy to train, not advanced methods to train the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2cAaREcIFv6"
      },
      "source": [
        "### 3.1 Adapter methods\n",
        "\n",
        "* Soft vs hard prompting\n",
        "  * Normal prompting\n",
        "  * Prefix tuning\n",
        "  * Adapter layers\n",
        "  * LLAMA adapter (history of it)\n",
        "* LoRA\n",
        " * QLoRA\n",
        "  * Quantization  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78-8y8ecWGQ"
      },
      "source": [
        "### 3.2 Deep dive into LoRA\n",
        "\n",
        "* Fine tune a very large model on a specific dataset with your own custom LoRA code written\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76Yc926IcbuL"
      },
      "source": [
        "## 4.0  RLHF; Multimodal Transformers; and other relevant developments related to LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F2_DQ7uciV6"
      },
      "source": [
        "Link to it and provide more details, but do not impliment. Requires to much background to FULLY grasp and build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## **Conclusion**\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:**\n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:**\n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2023)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "OIZvkhfRz9Jz",
        "outputId": "c6575aed-5030-4ee5-c6ea-2466cb790df4"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/Cg9aoa7czoZCYqxF7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6EqhIg1odqg0",
        "e9NW58_3hAg2",
        "o1ndpYE50BpG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
