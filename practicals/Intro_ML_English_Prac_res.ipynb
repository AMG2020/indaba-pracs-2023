{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Hands-on Introduction to Machine Learning**\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230706133033/An-introduction-to-Machine-Learning-01.webp\" width=\"60%\" />\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/5-feat-introduction-to-ml-practical/practicals/Intro_ML_English_Prac_res.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [Change colab link to point to prac.]\n",
        "\n",
        "© Deep Learning Indaba 2023. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "\n",
        "**Reviewers:**\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "In this tutorial, we will explore the fundamentals of machine learning. We will learn how to build and train a machine learning classifier. We will familarise ourselves with the concepts of loss functions and optimization.\n",
        "\n",
        "**Topics:**\n",
        "\n",
        "Content: Supervised learning, Neural Networks, Numerical Computing\n",
        "\n",
        "Level: <font color='grey'>`Beginner`\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Understand the basics of Machine learning.\n",
        "- Train a linear Regession model.\n",
        "- Train a neural network for multi-class classification.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html).\n",
        "\n",
        "**Outline:**\n",
        "\n",
        ">[Hands-on Introduction to Machine Learning](#scrollTo=m2s4kN_QPQVe)\n",
        "\n",
        ">>[What is machine learning](#scrollTo=FVw9C8GugkAj)\n",
        "\n",
        ">>>[1.1 Types of machine learning problems - Beginner](#scrollTo=v2k8dcYR9Hvb)\n",
        "\n",
        ">>>[1.2 Meeting JAX - Beginner](#scrollTo=742JhcnAxTof)\n",
        "\n",
        ">>>[JAX and NumPy - Differences ❌](#scrollTo=etSsvtmQz9L9)\n",
        "\n",
        ">>>>[JAX arrays are immutable, while NumPy arrays are not.](#scrollTo=mgu5mHNZ1Esn)\n",
        "\n",
        ">>>>[Randomness in NumPy vs JAX](#scrollTo=Ik_8oN9m1_zS)\n",
        "\n",
        ">>>>>[In Numpy, PRNG is based on a global state.](#scrollTo=jaWE-uiW2G4p)\n",
        "\n",
        ">>>>>[In JAX, PRNG is explicit.](#scrollTo=eNQW9JwK4Yp6)\n",
        "\n",
        ">>>[1.2 Data - Beginner](#scrollTo=1KK-dbRw730T)\n",
        "\n",
        ">>>>[Visualising the data](#scrollTo=w8g4rsOy_86b)\n",
        "\n",
        ">>[Regression](#scrollTo=e9NW58_3hAg2)\n",
        "\n",
        ">>>[2.1 Linear regression - Beginner](#scrollTo=bA_2coZvhAg3)\n",
        "\n",
        ">>>>[Model representation - Beginner](#scrollTo=kRQihf7uLExw)\n",
        "\n",
        ">>>>[Loss function and optimization - Beginner](#scrollTo=hNJgESq_LP4R)\n",
        "\n",
        ">>>>[Training the model using Jax - Beginner](#scrollTo=q9dQh9DdLYPE)\n",
        "\n",
        ">>>[2.2 Non linear regression and neural networks  - Beginner](#scrollTo=BKtMEnRkhAg9)\n",
        "\n",
        ">>>>[Model representation - Beginner](#scrollTo=XStsgHB2MarI)\n",
        "\n",
        ">>>>[Activation functions - Beginner](#scrollTo=fkpytbBzMvMj)\n",
        "\n",
        ">>>>[Building a simple neural network model with Jax - Beginner](#scrollTo=3Wrxt2orM7sk)\n",
        "\n",
        ">>[Classification](#scrollTo=fbTsk0MdhAhC)\n",
        "\n",
        ">>>[3.1 Logistic regression - Beginner](#scrollTo=wMgxJU0TOX6O)\n",
        "\n",
        ">>>>[Logits and sigmoid activation function - Beginner](#scrollTo=SE1L3rmaO4UP)\n",
        "\n",
        ">>>>[Extending to Multi-class classification](#scrollTo=HjQNjDC2cG2t)\n",
        "\n",
        ">>>>[Building a simple neural network for classification - Beginner](#scrollTo=EcXXE56hPOhK)\n",
        "\n",
        ">>>>[Training the model - Beginner](#scrollTo=GcSvlekHPWxW)\n",
        "\n",
        ">>>>[Evaluating the model - Beginner](#scrollTo=sGPvmGWkP1fT)\n",
        "\n",
        ">>[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">>[Feedback](#scrollTo=o1ndpYE50BpG)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Suggested experience level in this topic:**\n",
        "\n",
        "| Level         | Experience                            |\n",
        "| --- | --- |\n",
        "`Beginner`      | It is my first time being introduced to this work. |\n",
        "`Intermediate`  | I have done some basic courses/intros on this topic. |\n",
        "`Advanced`      | I work in this area/topic daily. |"
      ],
      "metadata": {
        "id": "952qogb79nnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title **Paths to follow:** What is your level of experience in the topics presented in this notebook? (Run Cell)\n",
        "experience = \"beginner\" #@param [\"beginner\", \"intermediate\", \"advanced\"]\n",
        "\n",
        "sections_to_follow=\"\"\n",
        "\n",
        "if experience == \"beginner\":\n",
        "  sections_to_follow=\"Introduction -> 1.1 Subsection -> 2.1 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"intermediate\":\n",
        "  sections_to_follow=\"Introduction -> 1.2 Subsection -> 2.2 Subsection -> Conclusion -> Feedback\"\n",
        "elif experience == \"advanced\":\n",
        "  sections_to_follow=\"Introduction -> 1.3 Subsection -> 2.3 Subsection -> Conclusion -> Feedback\"\n",
        "\n",
        "print(f\"Based on your experience, it is advised you follow these -- {sections_to_follow} sections. Note this is just a guideline.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YBdDHcI_ArCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "from typing import NamedTuple, Any\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVw9C8GugkAj"
      },
      "source": [
        "## **What is machine learning**\n",
        "In the last two decades, the field of artificial intelligence (AI) has transcended from being mainly used by computer scientists, mathematicians, and physicists to being applied in nearly every domain. It is currently used by almost every literate human being in one form or another. Applications like YouTube and Netflix, which we use on a daily basis, utilize AI to suggest content that we may like.\n",
        "<center>\n",
        "<img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/AIvsML.png\" width=\"80%\" />\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI is the ability to develop smart systems. Machine learning is subset of AI techniques that learns from data using statiscal modelling techniques. Deep Learning is a subset of machine learning that uses artificail neural networks for modelling."
      ],
      "metadata": {
        "id": "6pRWdqAi7FTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Types of machine learning problems - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "Machine learning is based on learning from data. When formulating a machine learning problem, the first question we need to ask ourselves is what type of learning task do we have? Broadly speaking, machine learning tasks can be classified into three categories.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.researchgate.net/publication/354960266/figure/fig1/AS:1075175843983363@1633353305883/The-main-types-of-machine-learning-Main-approaches-include-classification-and.png\" width=\"80%\" />\n",
        "</center>\n",
        "\n",
        "<font color='red'>Supervised Learning</font>: In supervised learning, the algorithm is provided with a labeled dataset, where each input data point is associated with the correct output (label). The goal is to learn a mapping from inputs to outputs based on the training data so that the model can make accurate predictions on new unseen data.\n",
        "\n",
        "<font color='red'>Unsupervised Learning</font>: In unsupervised learning, the algorithm is given an unlabeled dataset, and the goal is to find patterns, structures, or relationships within the data without explicit guidance. The algorithm tries to group similar data points or reduce the dimensionality of the data to reveal underlying structures.\n",
        "\n",
        "<font color='red'>Reinforcement Learning</font>: In reinforcement learning, the algorithm learns to make decisions through interactions with an environment. The learner (agent) receives feedback in the form of rewards or penalties based on its actions, which enables it to learn the best strategy to maximize the cumulative reward over time.\n",
        "\n",
        "**Excerise 1.1**: Can you identify to which of the categories the following task belong.\n",
        "  - Teaching a robot how to walk.\n",
        "  - Weather prediction.\n",
        "  - Spam emails classification.\n",
        "  - Teach a computer how to play chess.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v2k8dcYR9Hvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Meeting JAX - <font color='blue'>`Beginner`</font>\n",
        "<font color='red'> Modify or remove this Jax section </font>\n",
        "\n",
        "Throughout this tutorial, we will use the [JAX](https://jax.readthedocs.io/en/latest/) framework to illustrate the different concepts we will discuss. Jax is very similar to numpy but they are some important minor differences we need to be aware of."
      ],
      "metadata": {
        "id": "742JhcnAxTof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarities between JAX and Numpy"
      ],
      "metadata": {
        "id": "T221aJAVyjvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create NumPy arrays\n",
        "np_array1 = np.array([1, 2, 3])\n",
        "np_array2 = np.array([4, 5, 6])\n",
        "\n",
        "# Create JAX arrays\n",
        "jax_array1 = jnp.array([1, 2, 3])\n",
        "jax_array2 = jnp.array([4, 5, 6])\n",
        "\n",
        "# Element-wise addition using NumPy\n",
        "np_result = np_array1 + np_array2\n",
        "print(\"NumPy result:\", np_result)\n",
        "\n",
        "# Element-wise addition using JAX\n",
        "jax_result = jax_array1 + jax_array2\n",
        "print(\"JAX result:\", jax_result)"
      ],
      "metadata": {
        "id": "Co610zXhygak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Array concatenation using NumPy\n",
        "np_array1 = np.array([1, 2, 3])\n",
        "np_array2 = np.array([4, 5, 6])\n",
        "np_concatenated = np.concatenate([np_array1, np_array2])\n",
        "\n",
        "# Array concatenation using JAX\n",
        "jax_array1 = jnp.array([1, 2, 3])\n",
        "jax_array2 = jnp.array([4, 5, 6])\n",
        "jax_concatenated = jnp.concatenate([jax_array1, jax_array2])"
      ],
      "metadata": {
        "id": "o5i-RM66zYz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Element-wise functions using NumPy\n",
        "np_array = np.array([0, np.pi/2, np.pi])\n",
        "np_sin = np.sin(np_array)\n",
        "\n",
        "# Element-wise functions using JAX\n",
        "jax_array = jnp.array([0, jnp.pi/2, jnp.pi])\n",
        "jax_sin = jnp.sin(jax_array)"
      ],
      "metadata": {
        "id": "fz73kSvDzojV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduction operations using NumPy\n",
        "np_array = np.array([1, 2, 3])\n",
        "np_sum = np.sum(np_array)\n",
        "np_mean = np.mean(np_array)\n",
        "\n",
        "# Reduction operations using JAX\n",
        "jax_array = jnp.array([1, 2, 3])\n",
        "jax_sum = jnp.sum(jax_array)\n",
        "jax_mean = jnp.mean(jax_array)"
      ],
      "metadata": {
        "id": "yVj6HoN4z5KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX and NumPy - Differences ❌\n",
        "\n",
        "Although JAX and NumPy have some similarities, they do have some important differences:\n",
        "- Jax arrays are **immutable** (they can't be modified after they are created).\n",
        "- The way they handle **randomness** -- JAX handles randomness explicitly.\n",
        "\n"
      ],
      "metadata": {
        "id": "etSsvtmQz9L9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JAX arrays are immutable, while NumPy arrays are not.\n",
        "\n",
        "JAX and NumPy arrays are often interchangeable, **but** Jax arrays are **immutable** (they can't be modified after they are created). Allowing mutations makes transforms difficult and violates conditions for [pure functions](https://en.wikipedia.org/wiki/Pure_function)."
      ],
      "metadata": {
        "id": "mgu5mHNZ1Esn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see this in practice by changing the number at the beginning of an array."
      ],
      "metadata": {
        "id": "rf2bVtWr1QYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy: mutable arrays\n",
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)"
      ],
      "metadata": {
        "id": "aonuiSN81Ok5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try this in JAX."
      ],
      "metadata": {
        "id": "_bikd7W91eQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX: immutable arrays\n",
        "# Should raise an error.\n",
        "try:\n",
        "    x = jnp.arange(10)\n",
        "    x[0] = 10\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ],
      "metadata": {
        "id": "Kt9yRXan1mMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it fails! We can't mutate a JAX array once it has been created. To update JAX arrays, we need to use [helper functions](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) that return an updated copy of the JAX array.\n",
        "\n",
        "Instead of doing this `x[idx] = y`, we need to do this `x = x.at[idx].set(y)`."
      ],
      "metadata": {
        "id": "YHRr6uKH1sQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(10)\n",
        "new_x = x.at[0].set(10)\n",
        "print(f\" new_x: {new_x} original x: {x}\")"
      ],
      "metadata": {
        "id": "XLL6d2p81xue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note here that `new_x` is a copy and that the original `x` is unchanged."
      ],
      "metadata": {
        "id": "w6ibwTBj14gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Randomness in NumPy vs JAX\n",
        "\n",
        "JAX is more explicit in Pseudo Random Number Generation (PRNG) than NumPy and other libraries (such as TensorFlow or PyTorch). [PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) is the process of algorithmically generating a sequence of numbers, which *approximate* the properties of a sequence of random numbers.  \n",
        "\n",
        "Let's see the differences in how JAX and NumPy generate random numbers."
      ],
      "metadata": {
        "id": "Ik_8oN9m1_zS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In Numpy, PRNG is based on a global `state`.\n",
        "\n",
        "Let's set the initial seed."
      ],
      "metadata": {
        "id": "jaWE-uiW2G4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "prng_state = np.random.get_state()"
      ],
      "metadata": {
        "id": "wa5e2_gb2Jl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9X10jhocGaS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Helper function to compare prng keys (Run Cell)\n",
        "def is_prng_state_the_same(prng_1, prng_2):\n",
        "    \"\"\"Helper function to compare two prng keys.\"\"\"\n",
        "    # concat all elements in prng tuple\n",
        "    list_prng_data_equal = [(a == b) for a, b in zip(prng_1, prng_2)]\n",
        "    # stack all elements together\n",
        "    list_prng_data_equal = np.hstack(list_prng_data_equal)\n",
        "    # check if all elements are the same\n",
        "    is_prng_equal = all(list_prng_data_equal)\n",
        "    return is_prng_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a few samples from a Gaussian (normal) Distribution and check if PRNG keys/global state change."
      ],
      "metadata": {
        "id": "yCHvH8DZ3Aro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"sample 1 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 2 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 3 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")"
      ],
      "metadata": {
        "id": "KSixAoOQ2QU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy's global random state is updated every time a random number is generated, so *sample 1 != sample 2 != sample 3*.\n",
        "\n",
        "Having the state automatically updated, makes it difficult to handle randomness in a **reproducible** way across different threads, processes and devices."
      ],
      "metadata": {
        "id": "dlwT1pZH4M7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In JAX, PRNG is explicit.\n",
        "\n",
        "In JAX, for each random number generation, you need to explicitly pass in a random key/state.\n",
        "\n",
        "Passing the same state/key results in the same number being generated. This is generally undesirable."
      ],
      "metadata": {
        "id": "eNQW9JwK4Yp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ],
      "metadata": {
        "id": "Qd0PAWvf4KtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate different and independent samples, you need to manually split the keys."
      ],
      "metadata": {
        "id": "iUpdRj074oYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> new key and subkey\n",
        "new_key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the new key for future splits.\n",
        "# It doesn't really matter which key we keep and which one we use immediately.\n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> new key2 and subkey\n",
        "new_key2, subkey = random.split(new_key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ],
      "metadata": {
        "id": "82xoxsrF4vls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using JAX, we can more easily reproduce random number generation in parallel across threads, processes, or even devices by explicitly passing and keeping track of the prng key (without relying on a global state that automatically gets updated)."
      ],
      "metadata": {
        "id": "C2wiRBEo5OQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Data - <font color='blue'>`Beginner`</font>\n",
        "At the end of this tutorial we build a digit classifier for the popular mnist digit dataset. This dataset consist of images of handwritten digits. Our task is build a classifier that predicts the correct digit for each image.\n",
        "\n",
        "Before diving into the different machine learning concepts, let's load the data and visualise few of the images."
      ],
      "metadata": {
        "id": "1KK-dbRw730T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualising the data\n",
        "We will use [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_openml.html) to load the data."
      ],
      "metadata": {
        "id": "w8g4rsOy_86b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST digits dataset\n",
        "mnist = fetch_openml(name='mnist_784', version=1, as_frame=False)\n",
        "# Extract the data and labels\n",
        "images, labels = mnist.data, mnist.target\n",
        "\n",
        "# Convert labels to integers (they are originally stored as strings)\n",
        "labels = labels.astype(int)\n",
        "\n",
        "# Print shapes of the data\n",
        "print(images.shape, labels.shape)\n",
        "\n",
        "# Verify that we have 10 classes\n",
        "print(len(np.unique(labels)))\n",
        "\n",
        "# Print the min and max values of the images\n",
        "print(np.min(images), np.max(images))"
      ],
      "metadata": {
        "id": "jGpx7d-nBWK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consist of 70000 gray images. Each image has flattened to 1 dimensional array of size 784 from it's original 28 x 28 shape. We will use the code in the next cell to randomly select some of the images and display them. Note that each time you run the below cell you will get different images displayed because we randomly sample everytime."
      ],
      "metadata": {
        "id": "p_CflvadFKwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot images\n",
        "def plot_images(images, labels):\n",
        "  rows = 4\n",
        "  cols = 6\n",
        "  num_images = images.shape[0]\n",
        "\n",
        "  fig, axes = plt.subplots(rows, cols, figsize=(8, 8))\n",
        "\n",
        "  for ir in range(rows):\n",
        "    for ic in range(cols):\n",
        "      sample = np.random.randint(0, num_images)\n",
        "      image = images[sample].reshape(28, 28)\n",
        "      axes[ir, ic].imshow(image, cmap='gray')\n",
        "      axes[ir, ic].axis('off')\n",
        "      axes[ir, ic].set_title(f\"Digit {labels[sample]}\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_images(images, labels)"
      ],
      "metadata": {
        "id": "wOiX4ocqDteQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excerise:** Run the cell above multiple times and look at the images. Can you idenity some images that maybe difficult to classify?"
      ],
      "metadata": {
        "id": "tfmO5h5WJHK3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **Regression**\n",
        "Before we build our digit classfier, we need to first grabs some key ML concepts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper function (plot 2D classification illustration dataset)\n",
        "def plot_dataset(inputs, labels):\n",
        "  # Plot the given 2D inputs and labels using Matplotlib.\n",
        "  plt.scatter(\n",
        "      inputs[:, 0], inputs[:, 1],\n",
        "      c = ['red' if label == 0 else 'blue' if label == 1 else 'green' for label in labels])\n",
        "\n",
        "  xval = np.array([np.min(inputs[:,0]), np.max(inputs[:,0])])\n",
        "  yval = np.array([np.max(inputs[:,1]), np.min(inputs[:,1])])\n",
        "  # Plot the random decision boundary\n",
        "  plt.plot(xval, yval, c='black', label='Decision line')\n",
        "  plt.axis('equal')\n",
        "\n",
        "  plt.xlabel('x1')\n",
        "  plt.ylabel('x2')\n",
        "  plt.legend()"
      ],
      "metadata": {
        "id": "PfbD0ElAOZ9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below we display some randomly generated data for two categories displayed red and blue. Imagine we wanted to build a classifier that could tell us to which of the category each of green points belong. One intutive approach we can use for this is find a decision line or boundary we can use to separate the different points. The process of finding such a decision line is what we refer to as **regression** i.e. finding a function that maps a set of numerical values called **features** to another set of numerical values called **labels**.  If our objective is to fit a straight line, the process is referred to as **linear regression**."
      ],
      "metadata": {
        "id": "1ME59aXJRwE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate the Dataset  {run: \"auto\"}\n",
        "# Define the centre(s) of the points\n",
        "centre = 1.8    #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "\n",
        "points_in_class = 20  # How many points we want per class\n",
        "new_p = 2 # extra points per class\n",
        "\n",
        "# A fixed random seed is a common \"trick\" used in ML that allows us to recreate\n",
        "# the same data when there is a random element involved.\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate random points in the \"red\" class\n",
        "red_inputs = np.random.normal(loc=centre, scale=1.0, size=[points_in_class+new_p, 2])\n",
        "# Generate random points in the \"blue\" class\n",
        "blue_inputs = np.random.normal(loc=-centre, scale=1.0, size=[points_in_class+new_p, 2])\n",
        "# Put these together\n",
        "inputs = np.concatenate((red_inputs, blue_inputs), axis=0)\n",
        "\n",
        "# The class (label) is 1 for red or 0 for blue\n",
        "red_labels = np.ones(points_in_class)\n",
        "blue_labels = np.zeros(points_in_class)\n",
        "green_labels = np.ones((new_p*2))*2\n",
        "labels = np.concatenate((red_labels, green_labels, blue_labels), axis=0)\n",
        "\n",
        "# num_data_points is the total data set size\n",
        "num_data_points = 2 * points_in_class\n",
        "\n",
        "plot_dataset(inputs, labels)"
      ],
      "metadata": {
        "id": "kDix480DPCgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excerices**: Play around with the slider to modify the generated. Is every data generate easily separable?"
      ],
      "metadata": {
        "id": "RW8dule4Yoml"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA_2coZvhAg3"
      },
      "source": [
        "### 2.1 Linear regression - <font color='blue'>`Beginner`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model representation - <font color='blue'>`Beginner`</font>"
      ],
      "metadata": {
        "id": "kRQihf7uLExw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supposed we have a Dataset, with elements $\\mathbf{y}$ representing the labels and elements $\\mathbf{x}$ representing the features, our goal is to find a function $f$ that maps the features $\\mathbf{x} \\in \\mathbb{R}^d$ to the target variable $\\mathbf{y}$:\n",
        "$$ \\mathbf{y} \\, =\\, f(\\mathbf{x}). $$\n",
        "\n",
        "The linear regression model can be repressented mathematically as follows\n",
        "$$ f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b,$$\n",
        "where $\\mathbf{w} \\in \\mathbb{R}^d$. $\\mathbf{w}$ and $b$ are the parameters of the model usually refered to as weights. The term $b$ is commonly refered to as the bias and it can be included in $\\mathbf{w}$ by extending the feature vector $\\mathbf{x}$ with 1.\n",
        "\n",
        "An inutuive way to understand the equations above is to remember the equation of a line $$y \\, = \\, mx + c,$$ with the weights and the bias coressponding to the slope and the intecept, $m$ and $c$ respectively.    "
      ],
      "metadata": {
        "id": "GGTQetYgNBda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code demonstration"
      ],
      "metadata": {
        "id": "1SVA5XSjcfpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example implementation of a linear function computation.\n",
        "\n",
        "x_key = jax.random.PRNGKey(0)\n",
        "dim = 10\n",
        "x = jax.random.uniform(x_key, (dim,))\n",
        "\n",
        "w_key = jax.random.PRNGKey(1)\n",
        "w = jax.random.uniform(w_key, (dim,))\n",
        "\n",
        "b_key = jax.random.PRNGKey(2)\n",
        "b = jax.random.uniform(b_key, (1,))\n",
        "\n",
        "y = jnp.dot(w,x) + b  # can also be achieved using: y = x @ w + b"
      ],
      "metadata": {
        "id": "sBe3ytk5WyvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code task:**\n",
        "\n",
        "In the above cell we demonstrated how to compute the output of a linear model for single example in a datasets. When implementing a machine learning model we want to take advantage of linear algebra techniques and availaible computing resources to process a batch of data at once.\n",
        "\n",
        "Given a datasets $\\mathbf{X}$ with multiple examples stacked in matrix, write a function the applies a linear model to every example.\n"
      ],
      "metadata": {
        "id": "LH73KxFOdAMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this to generate the data\n",
        "# store the parameters in a dictionary\n",
        "params = dict(w=w, b=b)\n",
        "\n",
        "batch_size = 5\n",
        "X = jax.random.uniform(x_key, (batch_size, dim))\n",
        "data = jax.random.uniform(x_key, (batch_size,))"
      ],
      "metadata": {
        "id": "-Psjm_oQY_b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_model(params: Any, X: jnp.ndarray) -> jnp.ndarray:\n",
        "  # complete this code\n",
        "  w = params['w']\n",
        "  b = ... # update me. hint look at the above line for w\n",
        "\n",
        "  # compute m = f(x) here using w,b and X\n",
        "  m = ... # update me.\n",
        "\n",
        "  return m\n"
      ],
      "metadata": {
        "id": "Qh-eOfEdh-f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me to test your code\n",
        "\n",
        "def test_linear_model():\n",
        "  assert linear_model(params, X) == X @ w + b,\n",
        "  print(\"Nice! Your answer looks correct.\")\n",
        "\n",
        "test_linear_model()"
      ],
      "metadata": {
        "id": "1iL92mYSlCkM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution - linear model (Try not to peek until you've given it a good try!')\n",
        "def linear_model(params: Any, X: jnp.ndarray) -> jnp.ndarray:\n",
        "  # complete this code\n",
        "  w = params['w']\n",
        "  b = params['b']\n",
        "\n",
        "  # compute m = f(x) here using w,b and X\n",
        "  m = X @ w + b\n",
        "\n",
        "  return m"
      ],
      "metadata": {
        "id": "yFR2NtnTKgJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loss function and optimization - <font color='blue'>`Beginner`</font>\n"
      ],
      "metadata": {
        "id": "hNJgESq_LP4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss function**\n",
        "\n",
        "Whenever we are fitting a model to some data, we need a function to measure how well the model is performing. This function is called the **loss function**. It measures the amount of errors between our model and data."
      ],
      "metadata": {
        "id": "zV6jTdQN2NvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper code to plot errors\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of points to generate\n",
        "num_points = 20\n",
        "\n",
        "# Generate random points and lines\n",
        "points_x = np.random.rand(num_points)\n",
        "slope_random_line = np.random.rand()\n",
        "intercept_random_line = np.random.rand()\n",
        "\n",
        "points_y = slope_random_line*points_x + intercept_random_line + np.random.normal(0, 1, num_points)*0.15\n",
        "\n",
        "# Function to calculate distance from point (x, y) to the line y = mx + b\n",
        "def distance_to_line(x, y, m, b):\n",
        "    return abs(y - m * x - b) / np.sqrt(1 + m**2)\n",
        "\n",
        "# Calculate distances from each point to the random line\n",
        "distances = distance_to_line(points_x, points_y, slope_random_line, intercept_random_line)\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot the random line\n",
        "plt.plot(np.sort(points_x), slope_random_line * np.sort(points_x) + intercept_random_line, c='blue', label='Model')\n",
        "\n",
        "# Plot the points\n",
        "plt.scatter(points_x, points_y, c='red', label='Data', marker='o', s=50)\n",
        "\n",
        "# Plot lines from each point to the random line (in different colors)\n",
        "for i in range(num_points):\n",
        "    plt.plot([points_x[i], points_x[i]], [points_y[i], slope_random_line * points_x[i] + intercept_random_line],\n",
        "             c='green', alpha=0.7)\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title(\"Errors for each data point\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HI8NPCQn3gn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lost function tries to capture the total error we make for every data point. This is equivalent to summing up all the distances in green in figure above. This can be written mathematically as follows:\n",
        "$$ l(\\mathbb{θ}) \\, = \\, \\frac{1}{2m}∑_{i}({\\mathbf{y}_i - \\hat{\\mathbf{y}}_i})^2,$$\n",
        "where $\\mathbb{θ} = [\\mathbf{w}, b]$,  $l$ denotes the loss function, $\\hat{\\mathbf{y}}$ represents the predicited value by the model and $m$ is the total number of datapoints.\n",
        "\n",
        "Inserting the formula for a linear model, we obtain the following equation:\n",
        "\n",
        "$$l(\\mathbb{θ}) \\, = \\, \\frac{1}{2m}∑_{i}(\\mathbf{y}_i - \\mathbf{w}^T\\mathbf{x}_i - b)^2. $$\n",
        "<font color='blue'>Note: the factor of 2 is not necessarry and it is inserted in the expression for convience as it leads to a simple expressions when updating the model. </font>"
      ],
      "metadata": {
        "id": "Fj_aG5jG3b8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code task:** Implement the loss function above using the linear model function defined above.\n",
        "\n"
      ],
      "metadata": {
        "id": "coIwS0_5I-p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(params: Any, X: jnp.array, y: jnp.array):\n",
        "  # complete this code\n",
        "\n",
        "  m = linear_model(params, X)\n",
        "\n",
        "  loss = ... # your code here\n",
        "\n",
        "  return 0"
      ],
      "metadata": {
        "id": "C5ji0ISCJWQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @ Run to test my code\n",
        "def test_linear_loss_fn():\n",
        "  m = linear_model(params, X)\n",
        "  loss = 0.5*jnp.mean((m-y)**2)\n",
        "  assert loss_fn(params, X, data) == loss\n",
        "  print(\"Nice! Your answer looks correct.\")\n",
        "\n",
        "test_linear_loss_fn()"
      ],
      "metadata": {
        "id": "O0m0EZVqMUw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample solution (Try not to peek until you've given it a good try!')\n",
        "def loss_fn(params: Any, X: jnp.array, y: jnp.array):\n",
        "  # complete this code\n",
        "\n",
        "  m = linear_model(params, X)\n",
        "\n",
        "  loss = 0.5*jnp.mean((m-y)**2)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ehK1Y1pgZ83Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excercise:** quickly dicuss in groups how we can find the parameters, $\\mathbb{θ}$."
      ],
      "metadata": {
        "id": "NM7aMj_A75M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is optimization?**\n",
        "\n",
        "Optimisation as the name entails is the process of finding the optimal parameters of the model, i.e. the set of parameters that gives the minimum loss possible. Mathematically this is represented as follows\n",
        "\n",
        "$$\\underset{\\mathbf{\\theta}}{\\operatorname{argmin}} l(\\theta) \\,= \\, \\underset{\\mathbf{\\theta}}{\\operatorname{argmin}} \\frac{1}{2m}∑_{i}(\\mathbf{y}_i - \\mathbf{w}^T\\mathbf{x}_i - b)^2.$$"
      ],
      "metadata": {
        "id": "7SwSstfO9hml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper code (to visualise loss landscape)\n",
        "# Define the loss function (Mean Squared Error)\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Create a grid of weight (slope) and bias (intercept) values\n",
        "weight_values = np.linspace(-10, 10, 200)\n",
        "bias_values = np.linspace(-10, 10, 200)\n",
        "weight_grid, bias_grid = np.meshgrid(weight_values, bias_values)\n",
        "\n",
        "# Generate random data points for demonstration\n",
        "np.random.seed(0)\n",
        "x = np.linspace(0, 20, 200)\n",
        "y_true = 3 * x + 5 + 0.1 * np.random.normal(0, 1, 200)\n",
        "y_min = 3*x + 5\n",
        "minima = np.array([5, 3])\n",
        "loss_min = loss_function(y_true, y_min)\n",
        "\n",
        "# Compute the loss for each combination of weight and bias values\n",
        "loss_grid = np.zeros_like(weight_grid)\n",
        "for i in range(len(weight_values)):\n",
        "    for j in range(len(bias_values)):\n",
        "        y_pred = weight_values[i] * x + bias_values[j]\n",
        "        loss_grid[i, j] = loss_function(y_true, y_pred)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ax = plt.axes(projection='3d', elev=50, azim=-50)\n",
        "\n",
        "surf = ax.plot_surface(weight_grid, bias_grid, loss_grid, norm=LogNorm(), rstride=1, cstride=1,\n",
        "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
        "ax.plot(*minima, loss_min, 'r*', markersize=10)\n",
        "\n",
        "ax.set_xlabel('$w$')\n",
        "ax.set_ylabel('$b$')\n",
        "# Remove z-axis ticks and labels\n",
        "ax.set_zticks([])\n",
        "ax.set_zticklabels([])\n",
        "\n",
        "# Add color bar for reference\n",
        "cbar = plt.colorbar(surf, shrink=0.5, aspect=10)\n",
        "cbar.ax.set_ylabel('Loss')\n",
        "\n",
        "# ax.set_xlim((xmin, xmax))\n",
        "# ax.set_ylim((ymin, ymax))\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZjZAVVBK_Fsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'> How do we optimise </font>: The figure is a plot of the loss for values of $\\mathbf{w}$ and $b$ for a straight line. The minimum value is indicated with the star. Our goal in optimization is to identify this point.   \n",
        "\n",
        "A **brute force** approach will be to compute the loss function for a large range of possible parameters values and select the parameters with the minimum loss value. While such an approach can work for simple models with 1 or 2 parameters, this is a very tedious approach and the number of possible values to evaluate for large models makes such an approach impractical."
      ],
      "metadata": {
        "id": "kxUMu1HP_D-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient based optimization**\n",
        "\n",
        "The right approach is to use calculus. The derrivation of a function is known to be 0 at every turning point (maximum and minimum turning points). Hence to find $\\theta$ which minimises the loss, we need to solve the equation\n",
        "$$\\frac{\\partial l}{\\partial \\theta} \\,=\\, 0.$$\n",
        "\n",
        "**Excerise:** can you work out the the following expressions for the derivatives of the loss funnction.\n",
        "\n",
        "$$\\frac{∂ l}{\\partial \\mathbf{w}} = \\frac{1}{m}∑_{i}\\mathbf{x}_i(\\mathbf{w}^T\\mathbf{x}_i + b - \\mathbf{y}_i),$$\n",
        "\n",
        "$$\\frac{∂ l}{\\partial b} = \\frac{1}{m}∑_{i}(\\mathbf{w}^T\\mathbf{x}_i + b - \\mathbf{y}_i).$$"
      ],
      "metadata": {
        "id": "8h9sZ2vW97-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fortunately, we do not need to compute these analytical expressions or implement them ourselves from scratch. Machine learning frameworks like Jax, [Pytorch](https://pytorch.org/tutorials/) and [Tensorflow](https://www.tensorflow.org/) have highly optimised tools that will compute these for us. In the case of Jax we can use the function `jax.grad` to compute the derivative of a function with respect to its parameters."
      ],
      "metadata": {
        "id": "tq4p7ME6GWXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute derivatives with jax.grad\n",
        "# Computing derviatives with jax\n",
        "\n",
        "def quadratic_fn(x):\n",
        "  return x**2\n",
        "\n",
        "\n",
        "# Compute the derivative of quadratic_fn when x = 1\n",
        "grad_fx =  jax.grad(quadratic_fn)(1.0)\n",
        "assert grad_fx == 2.0\n",
        "print(f\"Gradient of the function x**2 at x = 1 is {grad_fx} as expected.\")"
      ],
      "metadata": {
        "id": "Yq2SMvz09fW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importantly Jax is able to compute the derivative in the same way even if our parameters passed in a different data type such as dictionary. For example we can compute the gradient of our loss function above as follows."
      ],
      "metadata": {
        "id": "LER9QyoXR_ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute derivatives of the loss_fn with respect to params\n",
        "\n",
        "grads = jax.grad(loss_fn)(params, X, data)\n",
        "print(grads)"
      ],
      "metadata": {
        "id": "UfVKXN_ToRcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another similar function in Jax is `jax.value_and_grad` which returns both the value of the function and the gradient."
      ],
      "metadata": {
        "id": "zl0_zYWxlGDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute derivatives of the loss_fn with respect to params\n",
        "\n",
        "loss, grads = jax.value_and_grad(loss_fn)(params, X, data)\n",
        "print(f\"loss: {loss}\")\n",
        "print(f\"gradients: {grads}\")"
      ],
      "metadata": {
        "id": "CzKxdMqIlsVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model using Jax - <font color='blue'>`Beginner`</font>"
      ],
      "metadata": {
        "id": "q9dQh9DdLYPE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1I8M0zkhAg3"
      },
      "source": [
        "**Gradient descent:**\n",
        "\n",
        "Now we have all the basic peices required to train a machine learning model. Recall the goal is to find the parameters that will make the derivative of the loss function zero. One popular algorithm for this in machine learning is the use of <font color='red'>grdient descent algorithm</font>. The algorithm works as follows:\n",
        "\n",
        "\n",
        "1.   Initialise the parameters with random values.\n",
        "2.   Loop for a number of iterations and during each iteration update the parameters using the followin formula.\n",
        "\n",
        "$$ \\mathbf{\\theta} = \\mathbf{\\theta} - η\\frac{∂l}{∂\\theta},$$\n",
        "$\\eta$ is called the learning rate. It is a hyperparemter of the model meaning we have to choose it before hand.\n",
        "\n",
        "Intuitively, if the current parameter are the optimal parameters then the gradients, $\\frac{\\partial l}{\\partial \\theta}$ will be zero and the parameters will stop changing. Hence one stopping critiria which we can use during training is checking when the parameters stop changing also known as convergence. When a single data point is use at every iteration to compute the gradient the algorithm is called <font color='red'>stochastic gradient descent</font>. When the entire training data is used the algorithm is called <font color='red'>batch gradient descent</font>. Traditionally the best approach is to use small chunks of the data at each step. This approach is called the <font color='red'>mini batch gradient descent</font>.\n",
        "\n",
        "Thus during training we will loop through the datasets for each iteration. When we loop through the entire dataset we called that an epoch. Hence another familiar hyperameter is the epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoFbSQWGhAg7"
      },
      "outputs": [],
      "source": [
        "# @title Code demonstration: Batch gradient decent implementation\n",
        "def batch_gradient_descent(loss_fn, params, training_data, val_data, learning_rate=0.01, num_epochs=20, batch_size=10):\n",
        "  \"\"\"Batch gradient descent basic jax implementation.\n",
        "\n",
        "  Args:\n",
        "    loss_fn\n",
        "      the loss function for our model.\n",
        "    params:\n",
        "      the initial parameters of the model.\n",
        "    training_data\n",
        "      a tuple with the features and targets for training.\n",
        "    val_data\n",
        "      a tuple with the features and targets for validation.\n",
        "    learning_rate\n",
        "      learning rate\n",
        "    num_epochs\n",
        "      number of epochs\n",
        "    batch_size:\n",
        "      size of every mini batch\n",
        "  \"\"\"\n",
        "\n",
        "  X_train, y_train = training_data\n",
        "  X_val, y_val = val_data\n",
        "\n",
        "  num_samples, num_features = X_train.shape\n",
        "\n",
        "  # Create empty list to store the training and validation loss.\n",
        "  loss_train = [] # training loss\n",
        "  loss_val  = [] # valisation loss\n",
        "\n",
        "  # Define a function that computes loss and gradients\n",
        "  loss_and_grad = jax.value_and_grad(loss_fn)\n",
        "\n",
        "  n_iter = 0 # number iterations\n",
        "  for epoch in range(num_epochs):\n",
        "    # Shuffle the data before every epoch\n",
        "    shuffled_indices = np.arange(num_samples)\n",
        "    np.random.shuffle(shuffled_indices)\n",
        "\n",
        "    for start_idx in range(0, num_samples, batch_size):\n",
        "      end_idx = start_idx + batch_size\n",
        "      if end_idx > num_samples:\n",
        "        end_idx = num_samples\n",
        "\n",
        "      batch_indices = shuffled_indices[start_idx:end_idx]\n",
        "      X_batch = X_train[batch_indices]\n",
        "      y_batch = y_train[batch_indices]\n",
        "\n",
        "      # Compute loss and gradients using value_and_grad\n",
        "      loss, grads = loss_and_grad(params, X_batch, y_batch)\n",
        "      loss_train.append(loss)\n",
        "\n",
        "      # Compute the validation loss\n",
        "      loss_v = loss_fn(params, X_val, y_val)\n",
        "      loss_val.append(loss_v)\n",
        "\n",
        "      # Update the parameters\n",
        "      params = jax.tree_map(lambda p, g: p -learning_rate*g, params, grads)\n",
        "\n",
        "      # update the iter count\n",
        "      n_iter += 1\n",
        "\n",
        "\n",
        "  # Plot training and validation loss\n",
        "  iters = range(1, n_iter+1)\n",
        "  plt.plot(iters, loss_train, label='Training Loss')\n",
        "  plt.plot(iters, loss_val, label='Validation Loss')\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gbBCDwQhAg8"
      },
      "outputs": [],
      "source": [
        "# @title Let's create some dummy data to test the code\n",
        "def create_dummy_data_linear_regression():\n",
        "  \"\"\"We will generate some random data using our linear function above to test the gradient decent implementation\"\"\"\n",
        "\n",
        "  num_samples = 200\n",
        "  num_features = 1  # Modify this to have more features\n",
        "\n",
        "  # Generate random X values in the range [0, 10] for each feature\n",
        "  key = jax.random.PRNGKey(0)\n",
        "  X = jax.random.uniform(key, (num_samples, num_features), minval=0, maxval=10)\n",
        "\n",
        "  # Generate y values based on a linear relationship with some noise\n",
        "  w = jnp.array([3.0])  # True coefficients for each feature\n",
        "  b = jnp.array([5.0])\n",
        "\n",
        "  params = dict(w=w, b=b)\n",
        "  y = linear_model(params, X)\n",
        "\n",
        "  # Add some noise\n",
        "  noise = jax.random.normal(key, shape=(num_samples,))*0.30\n",
        "  y = y + noise\n",
        "\n",
        "  # Step 2: Split the data into training and validation sets\n",
        "  train_fraction = 0.8\n",
        "  num_train_samples = int(train_fraction * num_samples)\n",
        "\n",
        "  # Shuffle the indices to randomly split the data\n",
        "  key, subkey = jax.random.split(key)\n",
        "  shuffled_indices = jax.random.permutation(subkey, jnp.arange(num_samples))\n",
        "\n",
        "  # Split the indices into training and validation sets\n",
        "  train_indices = shuffled_indices[:num_train_samples]\n",
        "  val_indices = shuffled_indices[num_train_samples:]\n",
        "\n",
        "  # Get the corresponding data points for training and validation sets\n",
        "  X_train, y_train = X[train_indices], y[train_indices]\n",
        "  X_val, y_val = X[val_indices], y[val_indices]\n",
        "\n",
        "  train_data = (X_train, y_train)\n",
        "  val_data = (X_val, y_val)\n",
        "\n",
        "  # create some random initial params\n",
        "  w_init = jax.random.normal(key, shape=(num_features,))\n",
        "  b_init = 0.0\n",
        "  initial_params = dict(w=w_init, b=b_init)\n",
        "\n",
        "  return train_data, val_data, initial_params\n",
        "\n",
        "\n",
        "def plot_linear_fit(params, X, y):\n",
        "  \"\"\"Overlay the data and fitted model\"\"\"\n",
        "\n",
        "  y_pred = linear_model(params, X).squeeze()\n",
        "  xval = X[:,0]\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.scatter(xval, y, label='Data', color='blue')\n",
        "  plt.plot(xval, y_pred, label='Fit', color='red')\n",
        "  plt.xlabel('X')\n",
        "  plt.ylabel('Y')\n",
        "  plt.legend()\n",
        "  plt.title('Scatter Points and Fitted Curve')\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data, initial_params = create_dummy_data_linear_regression()"
      ],
      "metadata": {
        "id": "CyvVrWO0YdDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = batch_gradient_descent(loss_fn, initial_params, train_data, val_data, learning_rate=0.01, num_epochs=2, batch_size=10)\n",
        "\n",
        "# plot the results on validation data\n",
        "Xval, yval = val_data\n",
        "plot_linear_fit(params, Xval, yval)"
      ],
      "metadata": {
        "id": "upnIeZ2Zaued"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model seems to be **underfitting** that data because the fitted line is going through very few of the points.\n",
        "\n",
        "**Excercise:** Dicusss with your neightbour how we can improve our model.\n",
        "\n",
        "**Code task:** Modify the learning rate, batch_size and the number epochs and observe their effects on the results."
      ],
      "metadata": {
        "id": "qzpzC6C25R2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Your code here\n",
        "learning_rate = ... # update here\n",
        "params = batch_gradient_descent(loss_fn, initial_params, train_data, val_data, learning_rate=learning_rate, num_epochs=20, batch_size=10)\n",
        "\n",
        "# plot the results on validation data\n",
        "Xval, yval = val_data\n",
        "plot_linear_fit(params, Xval, yval)"
      ],
      "metadata": {
        "id": "rB8DMH4Z7seC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Your code here\n",
        "batch_size = ... # update here\n",
        "params = batch_gradient_descent(loss_fn, initial_params, train_data, val_data, learning_rate=0.01, num_epochs=20, batch_size=batch_size)\n",
        "\n",
        "# plot the results on validation data\n",
        "Xval, yval = val_data\n",
        "plot_linear_fit(params, Xval, yval)"
      ],
      "metadata": {
        "id": "6pcP249L8Jhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKtMEnRkhAg9"
      },
      "source": [
        "### 2.2 Non linear regression and neural networks  - <font color='blue'>`Beginner`</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Helper code to plot non-linear data\n",
        "def plot_nonlinear_data():\n",
        "\n",
        "  x = np.linspace(-5, 5, 100)\n",
        "  y = np.cos(x)*x + x**2 + np.exp(-x/7)*3 + 0.3*x*np.sin(x)**4\n",
        "\n",
        "  plt.scatter(x, y)\n",
        "  plt.title(\"Example of non linear data\")\n",
        "  plt.show\n",
        "\n",
        "plot_nonlinear_data()"
      ],
      "metadata": {
        "id": "xNYVOlb-bUml",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression is a simple and powerful data inference method; however, it has limitations in capturing complex relationships. To model such complexities, we require more powerful models. For example no single line will perfectly fit the data in the above figure. Nonlinear models, such as polynomials, exponentials, and trigonometric functions, offer solutions to this problem.\n",
        "\n",
        "Before the explosion of deep learning techniques, fitting pre-defined functions to datasets was the goto machine learning approach. Examples of classcial algorithms using this approach include <font color='red'>support vector machines, naive Bayes, k-means clustering</font>, among others.\n",
        "\n",
        "In this section, we will introduce <font color='red'>neural networks</font> which is at the realm of deep learning techniques and recent sucess in machine learning."
      ],
      "metadata": {
        "id": "n2FIcY0XAzFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model representation - <font color='blue'>`Beginner`</font>\n"
      ],
      "metadata": {
        "id": "XStsgHB2MarI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![neural.jpg](data:image/jpeg;base64,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)\n",
        "\n",
        "            Image of a neural network with two hidden layers.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K7NYiMuVIUPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks are a powerful class of machine learning models inspired by how the human brain functions. Unlike linear regression, which applies a single transformation to the data, neural networks process input data through a series of transformations before reaching the final predictive layer. The term <font color='red'>Deep Learning</font> arises from the numerous transformations applied to the input data.\n",
        "\n",
        "In the image, the circles represent nodes, and the links connecting them represent the model's parameters."
      ],
      "metadata": {
        "id": "WAcehBE0J-G0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Activation functions - <font color='blue'>`Beginner`</font>\n",
        "Activation functions are essential in neural networks. The input to every node is a linear function of all the nodes from the previous layer. Without applying an activation function, also called a non-linearity, the entire neural network, no matter how many layers it has, reduces to a simple linear model. Thus, activation functions are used to break the linearity and ensure that we build a complex non-linear function.\n",
        "\n",
        "The equation for node \\(i\\) in layer \\(j\\) of a neural network can be expressed as follows:\n",
        "\n",
        "$$ \\text{Output}_{ij} = \\text{Activation Function} \\left( \\sum_{k=1}^{n} \\text{Weight}_{ijk} \\times \\text{Output}_{(j-1)k} + \\text{Bias}_{ij} \\right)$$\n",
        "\n",
        "Where:\n",
        "- $\\text{Output}_{ij}$ is the output of node $i$ in layer $j$.\n",
        "- $\\text{Weight}_{ijk}$ is the weight connecting node $i$ in layer $j$ to node $k$ in layer $j-1$.\n",
        "- $\\text{Output}_{(j-1)k}$ is the output of node $k$ in layer $j-1$.\n",
        "- $\\text{Bias}_{ij}$ is the bias term for node $i$ in layer $j$.\n",
        "- $\\text{Activation Function}$ is the chosen activation function that introduces nonlinearity to the output of the neuron.\n",
        "\n"
      ],
      "metadata": {
        "id": "fkpytbBzMvMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some poplular activation functions include:\n",
        "\n",
        "$\\text{ReLU})(x) \\, = \\, \\text{max}(0, x),$\n",
        "\n",
        "$\\text{tanh}(x)\\, = \\, \\frac{e^x - x^{-x}}{e^x + e^{-x}},$\n",
        "\n",
        "$\\text{Sigmoid}(x) \\, =\\, \\frac{1}{1+e^{-x}}$.\n",
        "\n",
        "Let implement the popular [ReLU](https://arxiv.org/https://arxiv.org/abs/1803.08375abs/1803.08375) activation function."
      ],
      "metadata": {
        "id": "qnXa2-o8RFK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of relu using simple python.\n",
        "\n",
        "def relu(x):\n",
        "  if x > 0 :\n",
        "    return x\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def plot_activation(act_fn, label=\"act_fn\"):\n",
        "    max_int = 5\n",
        "    # Generete 100 evenly spaced points from -max_int to max_int\n",
        "    x = np.linspace(-max_int, max_int, 1000)\n",
        "    y = np.array([act_fn(xi) for xi in x])\n",
        "    plt.plot(x, y, label=label)\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xticks(np.arange(min(x), max(x) + 1, 1))\n",
        "    plt.show()\n",
        "\n",
        "plot_activation(relu, label='ReLU')"
      ],
      "metadata": {
        "id": "17JUU2WIMOAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAQn8NUPhAg-"
      },
      "source": [
        "**Code Task:** Implement and plot another activation function of your choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Your code here\n",
        "def act_fn(x):\n",
        "    # type your code here\n",
        "\n",
        "    y = ...#\n",
        "    return y\n",
        "\n",
        "# Call the plotting function\n",
        "plot_activation(act_fn, label=....) # update this\n",
        "\n"
      ],
      "metadata": {
        "id": "bSCO__yzY5vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building a simple neural network model with Jax - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "Forturnately, they are different high level modules that can be used to develop deep neural networks and we don't have to implement everything from scratch. For example for jax based model we can use [haiku]( https://dm-haiku.readthedocs.io/en/latest/) and [flax](https://flax.readthedocs.io/en/latest/getting_started.html). Moreover advanced optimisation techniques can be implemented using [optax](https://optax.readthedocs.io/en/latest/). In this section we will use flax to implement a simple 4 layer neural network."
      ],
      "metadata": {
        "id": "3Wrxt2orM7sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code demonstration: building a simple neural network with flax\n",
        "# Code for a 4-layer neural network using haiku.\n",
        "# Here assume all the hidden layers have the same number of nodes.\n",
        "# We will use the same activation relu for all the layers except the last layer.\n",
        "\n",
        "class Network(nn.Module):\n",
        "  hidden_size: int\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # First layer with ReLU activation\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    # Second layer\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    # Third (output) layer with no activation applied\n",
        "    x = nn.Dense(self.output_size)(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "36LsHOHxjPuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell above is basic example of how we can define a neural network using flax. Each linear transformation is implemented using `nn.Dense` function. and we use `jax.nn.relu` to apply non linearities to the output of each layer."
      ],
      "metadata": {
        "id": "lCOgpUEsnvCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Code demonstration: intialising the model\n",
        "seed = 32\n",
        "input_size = 4\n",
        "hidden_size = 5\n",
        "output_size = 1\n",
        "\n",
        "# Calling the build_neural_network function and applying the required transformations\n",
        "model = Network(hidden_size, output_size)\n",
        "\n",
        "key = jax.random.PRNGKey(seed)\n",
        "dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "initial_params = model.init(key, dummy)\n",
        "print(initial_params)"
      ],
      "metadata": {
        "id": "dYMLCHd2nSS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell demonstrate how to create an instance of the model and get initial parameters. After creating an instance of the model using the `Network` class we call the `init` function with a `jax.random.PRNGKey` and some dummy inputs from which the shape of the parameters will be infered.  "
      ],
      "metadata": {
        "id": "0Q1_OKVVvR69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the model to some the data we need to call `model.apply` with the current parameters and the input data."
      ],
      "metadata": {
        "id": "KW6s0Pn3vwy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = model.apply(initial_params, dummy)\n",
        "print(z.shape, dummy.shape)"
      ],
      "metadata": {
        "id": "p3YtVINzqWP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbTsk0MdhAhC"
      },
      "source": [
        "## **Classification**\n",
        "Now that we are familiar with the fundamentals of model fitting and we know how to build neural networks, we will now focus on our original objective and build our mnist digit classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Logistic regression - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "Linear regression aims to find a function $f$ that maps our **inputs $x$**, where $x \\in \\mathbb{R}^d$ to the corresponding **output/target - $y$**, where $y \\in \\mathbb{R}^1$ (output is a single real number). Contrary to regression, the **output/target -$y$** can only take on certain values in logistic regression. When the **target** can only take on one of two values, the algorithm is called **Binary Clasification**. When we have more than two categories, it called a **Multi-class Classification**.\n",
        "\n",
        "Hence the aim of Logistic regression (in the Binary classification case) is to map **inputs $x$**, where $x \\in \\mathbb{R}^d$ to $y$, where $y \\in \\{0,1\\}$.\n",
        "\n",
        "For example, if we are building an image classifier for cats and dogs, 1 maybe used to represent the target values for cats and 0 for dogs."
      ],
      "metadata": {
        "id": "wMgxJU0TOX6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Logits and sigmoid activation function - <font color='blue'>`Beginner`</font>\n",
        "The target values for logistic regression problems are discrete values. It is not straigtfoward how to define a model function that outputs discrete values. Hence we design the model to output probabilities instead. Recall that probabilies only lie the range of values $[0,1]$, thus hence we need a function that maps the probabilities in the range $[0,1]$ to $(-∞, ∞)$. This function is called the [logit function](https://en.wikipedia.org/wiki/Logit) hence the name logistic regression.\n",
        "<br>\n",
        "<center>\n",
        " $$logit(p) = \\ln\\frac{p}{1-p},$$\n",
        "</center>\n",
        "where $p$ is the probability.\n",
        "\n",
        "**Let's plot the logit function.**"
      ],
      "metadata": {
        "id": "SE1L3rmaO4UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logit(p):\n",
        "    # computes the logit function\n",
        "    out = np.log(p/(1 - p + 1e-8)) # we add 1e-8 to just to avoid instances of division by 0\n",
        "    return out\n",
        "\n",
        "probs = np.random.uniform(0, 1, size=200)\n",
        "probs = np.sort(probs)\n",
        "plt.plot(probs, logit(probs), label=\"logit\")\n",
        "plt.legend(loc=\"best\")"
      ],
      "metadata": {
        "id": "TDTvbo9uguj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid function**\n",
        "\n",
        "The goal of logistic regression is to predict the logit for each of our inputs but for the sake of classification, we are interested in the probabilities directly. So we need to invert the logit function to obtained the probabilities. The inverse of the logit function is the **Sigmoid** activation function. Hence in order to build a neural network for binary classification\n",
        "<br>\n",
        "<center>\n",
        " $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Exercise:** show that the Sigmoid function is the inverse of the logit function."
      ],
      "metadata": {
        "id": "E03erVvqiq_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross entropy loss function**\n",
        "\n",
        "In binary classification we can only have one of two values for the targets, thus we can not use the same loss function as in previous regression example where the target output was continous. The most appropriate function model for the errors in logistic regression is the [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution). This leads to the following error function for logistic regression most often refered to as the cross entropy loss function\n",
        "<br>\n",
        "<center>\n",
        " $$ -y_i \\log(p_i) - (1-y_i) \\log (1-p_i),$$\n",
        "</center>\n",
        "where $p_i = \\sigma (z)$ with $z$ being the output of our model function.\n",
        "<br>\n",
        "\n",
        "**Code task**:\n",
        "1. Implement the sigmoid activation function.\n",
        "2. Implement the cross entropy loss function.\n"
      ],
      "metadata": {
        "id": "yAWfZN-7oadX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"sigmoid function 1/1+e^-x\"\"\"\n",
        "\n",
        "    prob = .... # update me\n",
        "\n",
        "    return prob"
      ],
      "metadata": {
        "id": "J9CiEUERAHJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me to test your code\n",
        "def test_sigmoid_fn():\n",
        "  x  = np.array([0.7, 0.3, 0.8, 0.2])\n",
        "  assert jnp.allclose(sigmoid(x), jax.nn.sigmoid(x)), \"Test failed!\"\n",
        "  print(\"Nice! Your answer looks correct!\")\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "YzKHb9DEAaMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title sigmoid solution (Try not to peek until you've given it a good try!')\n",
        "def sigmoid(x):\n",
        "  prob = 1/(1+jnp.exp(-x))\n",
        "\n",
        "  return prob"
      ],
      "metadata": {
        "id": "t75OVB1oBZOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(predictions, targets):\n",
        "    # you need to make sure we never have log of 0\n",
        "\n",
        "    # use sigmoid to compute the probs from the predicitions\n",
        "    probs = ... # update me\n",
        "\n",
        "    # your code here\n",
        "    loss = ... # update me\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "f-YTxxNnuOKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me to test your code\n",
        "def test_cross_entropy_loss():\n",
        "  predictions = np.array([0.7, 0.3, 0.8, 0.2])\n",
        "  targets = np.array([1, 0, 1, 0])\n",
        "\n",
        "  # Expected cross-entropy loss for the test data\n",
        "  expected_loss = 0.60669523\n",
        "\n",
        "  # Calculate the cross-entropy loss using the implemented function\n",
        "  computed_loss = cross_entropy_loss(predictions, targets)\n",
        "\n",
        "  assert jnp.isclose(computed_loss, expected_loss), \"Test failed!\"\n",
        "\n",
        "  # If the assert statement does not raise an exception, the test is passed.\n",
        "  print(\"Nice! Your anwser looks correct\")\n",
        "\n",
        "  return\n",
        "\n",
        "test_cross_entropy_loss()"
      ],
      "metadata": {
        "id": "WUW87CN4u9sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cross entropy loss solution (Try not to peek until you've given it a good try!')\n",
        "def cross_entropy_loss(preds, targets):\n",
        "    eps = 1e-15\n",
        "    probs = sigmoid(preds)\n",
        "\n",
        "    loss = -targets*jnp.log(probs+eps) - (1-targets)*jnp.log(1-probs+eps)\n",
        "\n",
        "    return jnp.mean(loss)"
      ],
      "metadata": {
        "id": "Y0VbBhG_u_UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extending to Multi-class classification"
      ],
      "metadata": {
        "id": "HjQNjDC2cG2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In multi-class classification, the machine learning model is designed to handle more than two classes, where each class represents a different category or label. For example, in the case of a single-digit classifier, there are 10 classes, each corresponding to a digit from 0 to 9.\n",
        "\n",
        "The model's output is typically a probability distribution over all possible classes, with each class having an associated probability. The dimensions of the output vector match the number of classes, so for a classifier with 3 different classes, the output vector will have a dimension of 3.\n",
        "\n",
        "To make a prediction, the model selects the class with the highest probability as the predicted class for the input data point."
      ],
      "metadata": {
        "id": "Dlr8evx-dCoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-hot encording**\n",
        "\n",
        "One-hot encoding is a common technique used to represent categorical variables, such as class labels, as binary vectors. In the case of multi-class classification with 3 classes, the targets are transformed into one-hot encoded vectors as follows:\n",
        "\n",
        "Class 1: [1, 0, 0]: This means the data point belongs to class 1, and the first element in the vector is set to 1, while the other elements are set to 0.\n",
        "\n",
        "Class 2: [0, 1, 0]: This means the data point belongs to class 2, and the second element in the vector is set to 1, while the other elements are set to 0.\n",
        "\n",
        "Class 3: [0, 0, 1]: This means the data point belongs to class 3, and the third element in the vector is set to 1, while the other elements are set to 0.\n",
        "\n",
        "We can use `jax.nn.one_hot` function to one-hot encode our data\n"
      ],
      "metadata": {
        "id": "BqLug2dpf8Zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An example using one hot encoding\n",
        "num_classes =  3\n",
        "targets = jnp.array([2, 0, 1])\n",
        "one_hot_targets = jax.nn.one_hot(targets, num_classes)\n",
        "print(one_hot_targets)"
      ],
      "metadata": {
        "id": "u_-jrxWPjVzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Discuss with your neighbour how we should define the loss function in the case of multi-class classification."
      ],
      "metadata": {
        "id": "ckIDzLRgj7_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**: The loss function for a multi-classification is computed similarly to that of binary classifier. However in this case we define the loss as the sum of the loss for each of the individual classes.\n",
        "<br>\n",
        "<center>\n",
        " $$ \\sum_c -y^c_i \\log(p^c_i),$$\n",
        "</center\n",
        "\n",
        "\n",
        ">\n",
        "where $p^c_i = \\text{softmax}(z) = \\frac{e^{z^c_i}}{\\sum_c e^{z^c_i}}$ with $z$ being the output of our model function.\n",
        "<br>\n",
        "The main difference here is that we use a $\\text{softmax}$ activation function instead of $\\text{sigmoid}$ as for the binary case.\n",
        "\n",
        "**Exercise**: Do your get the initution why this formula is similar to that for the binary case?"
      ],
      "metadata": {
        "id": "HMrwPge_lzVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code task**:\n",
        "  1. Implement the softmax activation function\n",
        "  2. Implement a cross entropy loss function for multiclass classification using softmax and one hot encording."
      ],
      "metadata": {
        "id": "2QnINsMrpTYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(logits):\n",
        "  \"\"\"Compute softmax: `exp(x)/sum(exp(x))`\n",
        "\n",
        "  Args:\n",
        "    logits: array of shape (num_samples, num_classes)\n",
        "\n",
        "  Return:\n",
        "    probs: array of shape (num_samples, num_classes)\n",
        "  \"\"\"\n",
        "\n",
        "  # your code here\n",
        "  # make sure you sum across the right axis\n",
        "\n",
        "  probs = ... # update me\n",
        "\n",
        "  return probs"
      ],
      "metadata": {
        "id": "4DWDGiKZpRX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me to test your code\n",
        "def test_softmax():\n",
        "  x = jnp.array([[1.0, 0.4, 0.3], [10.0, 4.6, 8.9]])\n",
        "  assert jnp.allclose(softmax(x), jax.nn.softmax(x, axis=-1))\n",
        "  print(\"Nice! Your answer looks correct.\")\n",
        "\n",
        "test_softmax()"
      ],
      "metadata": {
        "id": "UlBBgQafDsHQ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title softmax solution (Try not to peek until you've given it a good try!')\n",
        "def softmax(logits):\n",
        "  exp_logits = jnp.exp(logits)\n",
        "  return exp_logits / jnp.sum(exp_logits, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "5KKM6hKnxFSK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_softmax_loss(predictions, targets):\n",
        "    \"\"\"Compute the cross entropy softmax loss function\n",
        "\n",
        "      Args:\n",
        "        predictions: (num_samples, num_classes)\n",
        "        targets: (num_samples)\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the softmax probabilities\n",
        "    probs = ... # update me\n",
        "\n",
        "    num_classes = ... # update me\n",
        "\n",
        "    # One-hot encode the targets\n",
        "    targets_one_hot = ... # update me\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    eps = 1e-15\n",
        "    probs += eps # to avoid calling log with 0 values\n",
        "\n",
        "    loss = ... # update me\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "WIVjt6T4xTQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run me to test your code\n",
        "def test_cross_entropy_softmax_loss():\n",
        "  # Fixed predictions (logits) for each class\n",
        "  predictions = jnp.array([[1.5, 0.3, 2.7],\n",
        "        [0.8, 1.2, 3.1], [2.3, 1.7, 0.5],\n",
        "        [3.0, 0.5, 1.2], [0.2, 2.8, 1.0]])\n",
        "\n",
        "  # Fixed true class labels\n",
        "  targets = jnp.array([2, 1, 0, 0, 2])\n",
        "\n",
        "  # Expected loss computed manually\n",
        "  expected_loss = 1.0456787\n",
        "\n",
        "  # Compute the cross-entropy softmax loss using your implementation\n",
        "  loss = cross_entropy_softmax_loss(predictions, targets)\n",
        "\n",
        "  # Check if the computed loss matches the JAX built-in loss\n",
        "  assert jnp.allclose(loss, expected_loss), \"Loss value do not match!\"\n",
        "\n",
        "  print(\"Nice! Your answer looks correct.\")\n",
        "\n",
        "test_cross_entropy_softmax_loss()"
      ],
      "metadata": {
        "id": "8Tsbe8OazNq5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cross entropy softmax solution (Try not to peek until you've given it a good try!')\n",
        "def cross_entropy_softmax_loss(predictions, targets):\n",
        "    \"\"\"Compute the cross entropy softmax loss function\n",
        "\n",
        "      Args:\n",
        "        predictions: (num_samples, num_classes)\n",
        "        targets: (num_samples)\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the softmax probabilities\n",
        "    probs = softmax(predictions) # update me\n",
        "\n",
        "    num_classes = predictions.shape[-1]\n",
        "\n",
        "    # One-hot encode the targets\n",
        "    targets_one_hot = jax.nn.one_hot(targets, num_classes) # update me\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    eps = 1e-15\n",
        "    probs += eps # to avoid calling log with 0 values\n",
        "\n",
        "    loss_i = jnp.sum(-targets_one_hot*jnp.log(probs), axis=-1) # update me\n",
        "\n",
        "    return jnp.mean(loss_i)"
      ],
      "metadata": {
        "id": "WG5z8vOjfYiU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building a simple neural network for classification - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "In this section we will assemble all the pieces together and train a deep neural network for classification. Let's recall all the tools we mentionned intially that are necessary to train a machine learning model.\n",
        "\n",
        "1. Dataset: we need to have dataset which we will split into training and validation set in the ratio 80:20.\n",
        "2. A model function.\n",
        "3. A loss function.\n",
        "4. An optimisation algorithm.\n",
        "\n",
        "We will use a toy [sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html) dataset. We have not discussed data preprocessing but in practice we usually have to preprocess our datasets before using it for training. Such steps may include for example visualising the data to have intuition, identifying outliers, transforming and even dropping some features."
      ],
      "metadata": {
        "id": "EcXXE56hPOhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "from typing import NamedTuple, Any\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Let re-adapt our batch gradient descent function\n",
        "def batch_gradient_descent(loss_fn, params, training_data, val_data, learning_rate=0.01, num_epochs=20, batch_size=10):\n",
        "  \"\"\"Batch gradient descent basic jax implementation.\n",
        "\n",
        "  Args:\n",
        "    loss_fn\n",
        "      the loss function for our model.\n",
        "    params:\n",
        "      the initial parameters of the model.\n",
        "    training_data\n",
        "      a tuple with the features and targets for training.\n",
        "    val_data\n",
        "      a tuple with the features and targets for validation.\n",
        "    learning_rate\n",
        "      learning rate\n",
        "    num_epochs\n",
        "      number of epochs\n",
        "    batch_size:\n",
        "      size of every mini batch\n",
        "  \"\"\"\n",
        "\n",
        "  X_train, y_train = training_data\n",
        "  X_val, y_val = val_data\n",
        "\n",
        "  num_samples, num_features = X_train.shape\n",
        "\n",
        "  # Create empty list to store the training and validation loss.\n",
        "  loss_train = [] # training loss\n",
        "  loss_val  = [] # valisation loss\n",
        "\n",
        "  # Define a function that computes loss and gradients\n",
        "  loss_and_grad = jax.value_and_grad(loss_fn)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Shuffle the data before every epoch\n",
        "    shuffled_indices = np.arange(num_samples)\n",
        "    np.random.shuffle(shuffled_indices)\n",
        "\n",
        "    loss_train_epoch = []\n",
        "\n",
        "    for start_idx in range(0, num_samples, batch_size):\n",
        "      end_idx = start_idx + batch_size\n",
        "      if end_idx > num_samples:\n",
        "        end_idx = num_samples\n",
        "\n",
        "      batch_indices = shuffled_indices[start_idx:end_idx]\n",
        "      X_batch = X_train[batch_indices]\n",
        "      y_batch = y_train[batch_indices]\n",
        "      # Compute loss and gradients using value_and_grad\n",
        "      loss, grads = loss_and_grad(params, X_batch, y_batch)\n",
        "      loss_train_epoch.append(loss)\n",
        "\n",
        "      # Update the parameters\n",
        "      params = jax.tree_map(lambda p, g: p -learning_rate*g, params, grads)\n",
        "\n",
        "    # We need to turn the list in to an array before applying jnp.mean\n",
        "    mean_loss = jnp.mean(jnp.array(loss_train_epoch))\n",
        "    loss_train.append(mean_loss)\n",
        "\n",
        "    # Compute the validation loss at the end of the epoch\n",
        "    loss_v = loss_fn(params, X_val, y_val)\n",
        "    loss_val.append(loss_v)\n",
        "\n",
        "  # Plot training and validation loss\n",
        "  epochs = range(1, num_epochs+1)\n",
        "  plt.plot(epochs, loss_train, label='Training Loss')\n",
        "  plt.plot(epochs, loss_val, label='Validation Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.legend()\n",
        "\n",
        "  # Display the plot\n",
        "  plt.show()\n",
        "\n",
        "  return params\n",
        "\n",
        "# Load the mnist dataset\n",
        "def load_dataset(seed):\n",
        "  mnist = fetch_openml(name='mnist_784', version=1, as_frame=False)\n",
        "  # Extract the data and labels\n",
        "  images, labels = mnist.data, mnist.target\n",
        "\n",
        "  # These images consist of integer values from 0 to 255.0\n",
        "  # We scale the images to min and max of 1\n",
        "  x_max = 255.0\n",
        "  x_min = 0.0\n",
        "\n",
        "  images = (images - x_min)/(x_max - x_min)\n",
        "  images = images.astype(jnp.float32)\n",
        "  labels = labels.astype(jnp.float32)\n",
        "\n",
        "  # These are images of shape 28x28 which have been flatten to shape 784\n",
        "  X_train, X_test, y_train, y_test = train_test_split(\n",
        "      images, labels, test_size=0.2, train_size=0.8, random_state=seed\n",
        "  )\n",
        "\n",
        "  train_dataset = (X_train, y_train)\n",
        "  test_dataset = (X_test, y_test)\n",
        "\n",
        "  return train_dataset, test_dataset\n",
        "\n",
        "training_data, val_data = load_dataset(32)"
      ],
      "metadata": {
        "id": "II8gN_lTKoQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code task:**\n",
        "1. Build a neural network that outputs logits for each of the 10 classes.\n",
        "2. Initialse your model with some dummy input.\n",
        "3. Define your softmax cross entropy function."
      ],
      "metadata": {
        "id": "fcJMGW7HWkWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Your code here\n",
        "class Network(nn.Module):\n",
        "  hidden_size: int\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # First layer with ReLU activation\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    # Second layer\n",
        "    x = ...  # update me\n",
        "    x = ...  # update me\n",
        "\n",
        "    # Third (output) layer with no activation applied\n",
        "    x = nn.Dense(self.output_size)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# transform and intialiase the model.\n",
        "seed = 32\n",
        "input_size = 784 # remember our features are images of shape 28x28 flatten\n",
        "hidden_size = ... # update me\n",
        "output_size = ... # update me (hint: number classes)\n",
        "\n",
        "# instantiate the model\n",
        "model = Network... # update me\n",
        "\n",
        "key = jax.random.PRNGKey(seed)\n",
        "dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "initial_params = model.init(key, dummy)\n",
        "\n",
        "def softmax(logits):\n",
        "  exp_logits = jnp.exp(logits)\n",
        "  return exp_logits / jnp.sum(exp_logits, axis=-1, keepdims=True)\n",
        "\n",
        "\n",
        "def cross_entropy_softmax_loss(params, X, targets):\n",
        "    \"\"\"Compute the cross entropy softmax loss function\n",
        "\n",
        "      Args:\n",
        "        params: model parameters\n",
        "        X: features arrary (num_samples, num_features)\n",
        "        targets: (num_samples)\n",
        "    \"\"\"\n",
        "\n",
        "    # use the model to compute the predictions\n",
        "    preds = model.apply(params, X)\n",
        "\n",
        "    # Compute the softmax probabilities\n",
        "    probs = softmax(preds)\n",
        "\n",
        "    num_classes = preds.shape[-1]\n",
        "\n",
        "    # One-hot encode the targets\n",
        "    targets_one_hot = jax.nn.one_hot(targets, num_classes)\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    eps = 1e-15\n",
        "    probs += eps # to avoid calling log with 0 values\n",
        "\n",
        "    loss_i = jnp.sum(-targets_one_hot*jnp.log(probs), axis=-1)\n",
        "\n",
        "    return jnp.mean(loss_i)\n"
      ],
      "metadata": {
        "id": "DBOzpb2YXzvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample solution (Try not to peek until you've given it a good try!')\n",
        "class Network(nn.Module):\n",
        "  hidden_size: int\n",
        "  output_size: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    # First layer with ReLU activation\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    # Second layer\n",
        "    x = nn.Dense(self.hidden_size)(x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    # Third (output) layer with no activation applied\n",
        "    x = nn.Dense(self.output_size)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# transform and intialiase the model.\n",
        "seed = 67\n",
        "input_size = 784\n",
        "hidden_size = 100\n",
        "output_size = 10\n",
        "\n",
        "# Instantiate the model\n",
        "model = Network(hidden_size, output_size)\n",
        "\n",
        "key = jax.random.PRNGKey(seed)\n",
        "dummy = jnp.zeros((1, input_size), dtype=float)\n",
        "initial_params = model.init(key, dummy)\n",
        "\n",
        "def softmax(logits):\n",
        "  exp_logits = jnp.exp(logits)\n",
        "  return exp_logits / jnp.sum(exp_logits, axis=-1, keepdims=True)\n",
        "\n",
        "def cross_entropy_softmax_loss(params, X, targets):\n",
        "    \"\"\"Compute the cross entropy softmax loss function\n",
        "\n",
        "      Args:\n",
        "        params: model params\n",
        "        X: (num_features, num_classes)\n",
        "        targets: (num_samples)\n",
        "    \"\"\"\n",
        "\n",
        "    preds = model.apply(params, X)\n",
        "\n",
        "    # Compute the softmax probabilities\n",
        "    probs = softmax(preds)\n",
        "\n",
        "    num_classes = preds.shape[-1]\n",
        "\n",
        "    # One-hot encode the targets\n",
        "    targets_one_hot = jax.nn.one_hot(targets, num_classes)\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    eps = 1e-15\n",
        "    probs += eps # to avoid calling log with 0 values\n",
        "\n",
        "    loss_i = jnp.sum(-targets_one_hot*jnp.log(probs), axis=-1)\n",
        "\n",
        "    return jnp.mean(loss_i)\n"
      ],
      "metadata": {
        "id": "mBM7kwGjY4dr",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GcSvlekHPWxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code task:**\n",
        "  1. Call the `batch_gradient_descent` function to train the model.\n",
        "  2. Vary the `learning_rate` and the `batch_size` and observe the behaviour of the loss function."
      ],
      "metadata": {
        "id": "xed1mfr1cOzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = batch_gradient_descent ... # update me"
      ],
      "metadata": {
        "id": "lslWzLP0cciM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sample solution (Try not to peek until you've given it a good try!')\n",
        "params = batch_gradient_descent(cross_entropy_softmax_loss, initial_params, training_data, val_data, learning_rate=0.01, num_epochs=10, batch_size=1000)"
      ],
      "metadata": {
        "id": "WZGTHUrTdW2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating the model - <font color='blue'>`Beginner`</font>\n",
        "So far the only metric we have used to evaluate the performance of our model is has been the loss function. However when training a machine learning model, several other metrics can be used to acess the perfomance of the model. Furthermore, hyper-parameters are generally selected (fine-tuned) based on the most important metric we want to optimise.\n",
        "*   **Accuracy**: This the most common metric generally used in classification. It is measure for the proportion of the number of instances classified correctly. $$ \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
        "\n",
        "Other evaluation metrics sometimes used include Precision, Recall, F1 sore etc.\n"
      ],
      "metadata": {
        "id": "sGPvmGWkP1fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's use accuracy to evaluate the perfomance of our trained classifier.**\n",
        "\n",
        "Firstly let define a prediction function that transforms the model output to class. Recall the output of the classifier is logits which corresponds to unnormalised probabilities belonging to each class. Thus the predicted class is the one with the highest probability    \n",
        "\n",
        "**Code Task**\n",
        "  1. Implement the a function model predict that selects the appropriate class from the output of the model."
      ],
      "metadata": {
        "id": "-0htEsSspFQN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWMMpYSDhAhE"
      },
      "outputs": [],
      "source": [
        "# @title Your code here\n",
        "def model_predict(params, X):\n",
        "    \"\"\"Use the model for predicition\n",
        "\n",
        "      args:\n",
        "        params: model parameters\n",
        "        X: features array (num_samples, num_features)\n",
        "\n",
        "      return\n",
        "        pred: predicted class (num_samples, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # call the model to compute the logits\n",
        "    logits = model... # update me\n",
        "    preds = ... # update me: hint use jnp.argmax\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run to test your code\n",
        "def test_model_predict():\n",
        "    features = val_data[0][0:10]\n",
        "    preds = model_predict(params, features)\n",
        "    expected = jnp.argmax(model.apply(params, features), axis=-1)\n",
        "    assert jnp.array_equal(preds, expected), \"Failed! try again\"\n",
        "    print(\"Nice! Your answer looks correct.\")\n",
        "\n",
        "test_model_predict()"
      ],
      "metadata": {
        "id": "3u3PzTGmxENi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhJbRdW8DsHS",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Solution model predict (Try not to peek until you've given it a good try!')\n",
        "def model_predict(params, X):\n",
        "    \"\"\"Use the model for predicition\n",
        "\n",
        "      args:\n",
        "        params: model parameters\n",
        "        X: features array (num_samples, num_features)\n",
        "\n",
        "      return\n",
        "        pred: predicted class (num_samples, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # call the model to compute the logits\n",
        "    logits = model.apply(params, X)\n",
        "    preds = jnp.argmax(logits, axis=-1)\n",
        "\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compute the accuray of your model\n",
        "def compute_accuracy(data):\n",
        "  \"\"\"Compute the accuracy of the model\"\"\"\n",
        "\n",
        "  X, y_true = data\n",
        "  y_pred = model_predict(params, X).squeeze()\n",
        "\n",
        "  acc = sum(y_pred==y_true)/len(y_true)\n",
        "\n",
        "  return acc\n",
        "\n",
        "train_acc = compute_accuracy(training_data)\n",
        "val_acc = compute_accuracy(val_data)\n",
        "print(f\"The training accuracy is {train_acc} while the validation accuracy is {val_acc}\")"
      ],
      "metadata": {
        "id": "k-rRiOD2z5ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations, we've just trained a machine learning model for classification using the popular mnist digit dataset.\n",
        "\n",
        "**Excercise**:\n",
        "- Are you satisfied with the accuracy of your model?\n",
        "- What can you do to improve the accuracy of the model?"
      ],
      "metadata": {
        "id": "r5-yxk9e19q4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:**\n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:**\n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:**\n",
        "\n",
        "1. https://d2l.ai/chapter_linear-networks/linear-regression.html\n",
        "2. https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
        "3. https://www.simplilearn.com/tutorials/artificial-intelligence-tutorial/ai-vs-machine-learning-vs-deep-learning\n",
        "\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2023)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kAgB4cffVSlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/Cg9aoa7czoZCYqxF7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "742JhcnAxTof"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}